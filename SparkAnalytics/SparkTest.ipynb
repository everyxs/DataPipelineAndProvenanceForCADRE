{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Application application_1572457089359_0028 failed 2 times due to AM Container for appattempt_1572457089359_0028_000002 exited with  exitCode: -1000\n",
       "Failing this attempt.Diagnostics: [2019-11-04 16:41:22.498]File file:/N/u/yan30/Carbonate/.sparkStaging/application_1572457089359_0028/datanucleus-core-4.1.17.jar does not exist\n",
       "java.io.FileNotFoundException: File file:/N/u/yan30/Carbonate/.sparkStaging/application_1572457089359_0028/datanucleus-core-4.1.17.jar does not exist\n",
       "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:641)\n",
       "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:867)\n",
       "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:631)\n",
       "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:442)\n",
       "\tat org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:269)\n",
       "\tat org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:67)\n",
       "\tat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:414)\n",
       "\tat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:411)\n",
       "\tat java.security.AccessController.doPrivileged(Native Method)\n",
       "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
       "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
       "\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:411)\n",
       "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.doDownloadCall(ContainerLocalizer.java:242)\n",
       "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:235)\n",
       "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:223)\n",
       "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
       "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
       "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "For more detailed output, check the application tracking page: http://iuni2:8088/cluster/app/application_1572457089359_0028 Then click on links to logs of each attempt.\n",
       ". Failing the application.\n",
       "StackTrace: org.apache.spark.SparkException: Application application_1572457089359_0028 failed 2 times due to AM Container for appattempt_1572457089359_0028_000002 exited with  exitCode: -1000\n",
       "Failing this attempt.Diagnostics: [2019-11-04 16:41:22.498]File file:/N/u/yan30/Carbonate/.sparkStaging/application_1572457089359_0028/datanucleus-core-4.1.17.jar does not exist\n",
       "java.io.FileNotFoundException: File file:/N/u/yan30/Carbonate/.sparkStaging/application_1572457089359_0028/datanucleus-core-4.1.17.jar does not exist\n",
       "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:641)\n",
       "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:867)\n",
       "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:631)\n",
       "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:442)\n",
       "\tat org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:269)\n",
       "\tat org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:67)\n",
       "\tat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:414)\n",
       "\tat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:411)\n",
       "\tat java.security.AccessController.doPrivileged(Native Method)\n",
       "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
       "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
       "\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:411)\n",
       "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.doDownloadCall(ContainerLocalizer.java:242)\n",
       "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:235)\n",
       "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer$FSDownloadWrapper.call(ContainerLocalizer.java:223)\n",
       "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
       "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
       "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "For more detailed output, check the application tracking page: http://iuni2:8088/cluster/app/application_1572457089359_0028 Then click on links to logs of each attempt.\n",
       ". Failing the application.\n",
       "  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.waitForApplication(YarnClientSchedulerBackend.scala:95)\n",
       "  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)\n",
       "  at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:186)\n",
       "  at org.apache.spark.SparkContext.<init>(SparkContext.scala:511)\n",
       "  at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2549)\n",
       "  at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:944)\n",
       "  at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:935)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:935)\n",
       "  at org.apache.toree.kernel.api.Kernel$$anonfun$1.apply(Kernel.scala:428)\n",
       "  at org.apache.toree.kernel.api.Kernel$$anonfun$1.apply(Kernel.scala:428)\n",
       "  at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n",
       "  at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n",
       "  at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121)\n",
       "  at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n",
       "  at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n",
       "  at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.types.{StructType, StructField, StringType, DoubleType};\n",
    "import com.databricks.spark.xml._\n",
    "\n",
    "val sqlContext = new SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "sqlContext.sql(\"set spark.sql.caseSensitive=true\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "import java.io.File\n",
    "import scala.collection.JavaConversions._ // important for 'foreach'\n",
    "import org.apache.commons.io.FileUtils\n",
    "\n",
    "FileUtils.listFiles(new File(\"/N/dc2/projects/IUNI_MSAcademic/wos_raw/2017_old/\"), Array(\"foo\"), true).foreach{ f =>\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://iuni2:8020/data1/parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fs = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_383504919_58, ugi=hdfs (auth:SIMPLE)]]\n",
       "status = Array(HdfsLocatedFileStatus{path=hdfs://iuni2:8020/data1/parquet; isDirectory=true; modification_time=1572555443195; access_time=0; owner=hdfs; group=supergroup; permission=rwxr-xr-x; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false})\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array(HdfsLocatedFileStatus{path=hdfs://iuni2:8020/data1/parquet; isDirectory=true; modification_time=1572555443195; access_time=0; owner=hdfs; group=supergroup; permission=rwxr-xr-x; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.fs.{FileSystem, FileUtil, Path}\n",
    "\n",
    "val fs = FileSystem.get(sc.hadoopConfiguration)\n",
    "val status = fs.listStatus(new Path(\"/data1\"))\n",
    "status.foreach(x=> println(x.getPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 60, iuni4, executor 7): java.io.FileNotFoundException: File file:/N/dc2/projects/IUNI_MSAcademic/wos_raw/2017_old/WR_2015_20160330230947_CORE_0001.xml.gz does not exist\n",
       "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:641)\n",
       "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:867)\n",
       "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:631)\n",
       "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:442)\n",
       "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)\n",
       "\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:347)\n",
       "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:898)\n",
       "\tat com.databricks.spark.xml.XmlRecordReader.initialize(XmlInputFormat.scala:79)\n",
       "\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:203)\n",
       "\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:200)\n",
       "\tat org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:152)\n",
       "\tat org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:71)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:641)\n",
       "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:867)\n",
       "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:631)\n",
       "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:442)\n",
       "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)\n",
       "\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:347)\n",
       "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:898)\n",
       "\tat com.databricks.spark.xml.XmlRecordReader.initialize(XmlInputFormat.scala:79)\n",
       "\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:203)\n",
       "\tat org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:200)\n",
       "\tat org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:152)\n",
       "\tat org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:71)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2178)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n",
       "  at com.databricks.spark.xml.util.InferSchema$.infer(InferSchema.scala:111)\n",
       "  at com.databricks.spark.xml.XmlRelation$$anonfun$1.apply(XmlRelation.scala:42)\n",
       "  at com.databricks.spark.xml.XmlRelation$$anonfun$1.apply(XmlRelation.scala:42)\n",
       "  at scala.Option.getOrElse(Option.scala:121)\n",
       "  at com.databricks.spark.xml.XmlRelation.<init>(XmlRelation.scala:41)\n",
       "  at com.databricks.spark.xml.XmlRelation$.apply(XmlRelation.scala:29)\n",
       "  at com.databricks.spark.xml.DefaultSource.createRelation(DefaultSource.scala:73)\n",
       "  at com.databricks.spark.xml.DefaultSource.createRelation(DefaultSource.scala:51)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:317)\n",
       "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n",
       "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
       "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n",
       "  ... 48 elided\n",
       "Caused by: java.io.FileNotFoundException: File file:/N/dc2/projects/IUNI_MSAcademic/wos_raw/2017_old/WR_2015_20160330230947_CORE_0001.xml.gz does not exist\n",
       "  at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:641)\n",
       "  at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:867)\n",
       "  at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:631)\n",
       "  at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:442)\n",
       "  at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)\n",
       "  at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:347)\n",
       "  at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:898)\n",
       "  at com.databricks.spark.xml.XmlRecordReader.initialize(XmlInputFormat.scala:79)\n",
       "  at org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:203)\n",
       "  at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:200)\n",
       "  at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:152)\n",
       "  at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:71)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n",
       "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val sqlContext = new SQLContext(sc)\n",
    "val customSchema = StructType(Array(\n",
    "  StructField(\"UID\", StringType, nullable = true),\n",
    "  StructField(\"static_data\", StructType(Array( StructField(\"fullrecord_metadata\", StructType(Array(\n",
    "        //StructField(\"titles\", StructType(Array(StructField(\"title\", StringType, nullable = false)))),\n",
    "        //StructField(\"names\", StructType(Array(StructField(\"name\", StringType, nullable = false)))),\n",
    "        StructField(\"references\", StructType(Array(StructField(\"reference\", StructType(Array(StructField(\"UID\", StringType, nullable = false)))))))\n",
    "  ))))))))\n",
    "val df = sqlContext.read\n",
    "  .format(\"com.databricks.spark.xml\")\n",
    "  .option(\"rowTag\", \"REC\")\n",
    "  //.schema(customSchema)\n",
    "  .load(\"file:///N/dc2/projects/IUNI_MSAcademic/wos_raw/2017_old/WR_2015_20160330230947_CORE_0001.xml.gz\")\n",
    "\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: lastException: Throwable = null\n",
       "<console>:46: error: not found: value df\n",
       "       df.repartition(1000).write.mode(SaveMode.Overwrite).parquet(\"file:////N/dc2/projects/IUNI_MSAcademic/wos_raw/2017_old/parquet/\")\n",
       "       ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "df.repartition(1000).write.mode(SaveMode.Overwrite).parquet(\"file:////N/dc2/projects/IUNI_MSAcademic/wos_raw/2017_old/parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UID: string (nullable = true)\n",
      " |-- identifier: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _type: string (nullable = true)\n",
      " |    |    |-- _value: string (nullable = true)\n",
      " |-- p: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- doctype: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- _pubyear: long (nullable = true)\n",
      " |-- display_name: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WoS = [UID: string, _r_id_disclaimer: string ... 13 more fields]\n",
       "WoS2 = [UID: string, identifier: array<struct<_VALUE:string,_type:string,_value:string>> ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[UID: string, identifier: array<struct<_VALUE:string,_type:string,_value:string>> ... 4 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val WoS = spark.read.format(\"parquet\").load(\"/data1/parquet/\")\n",
    "val WoS2 = WoS.select(\"UID\",\"dynamic_data.cluster_related.identifiers.identifier\",\"static_data.abstracts.abstract.abstract_text.p\",\"static_data.summary.doctypes.doctype\",\n",
    "                       \"static_data.summary.pub_info._pubyear\",\"static_data.summary.names.name.display_name\")\n",
    "WoS2.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java.security.AccessControlException: access denied org.apache.derby.security.SystemPermission( \"engine\", \"usederbyinternals\" )\n",
      "\tat java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)\n",
      "\tat java.security.AccessController.checkPermission(AccessController.java:884)\n",
      "\tat org.apache.derby.iapi.security.SecurityUtil.checkDerbyInternalsPrivilege(Unknown Source)\n",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.startMonitor(Unknown Source)\n",
      "\tat org.apache.derby.iapi.jdbc.JDBCBoot$1.run(Unknown Source)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat org.apache.derby.iapi.jdbc.JDBCBoot.boot(Unknown Source)\n",
      "\tat org.apache.derby.iapi.jdbc.JDBCBoot.boot(Unknown Source)\n",
      "\tat org.apache.derby.jdbc.EmbeddedDriver.boot(Unknown Source)\n",
      "\tat org.apache.derby.jdbc.EmbeddedDriver.<clinit>(Unknown Source)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat java.lang.Class.newInstance(Class.java:442)\n",
      "\tat org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:47)\n",
      "\tat org.datanucleus.store.rdbms.connectionpool.DBCPConnectionPoolFactory.createConnectionPool(DBCPConnectionPoolFactory.java:50)\n",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)\n",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)\n",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)\n",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)\n",
      "\tat org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)\n",
      "\tat org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)\n",
      "\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)\n",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n",
      "\tat org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)\n",
      "\tat org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:420)\n",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)\n",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)\n",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n",
      "\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:610)\n",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:655)\n",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:419)\n",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:356)\n",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:317)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:77)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:137)\n",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)\n",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:688)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:654)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:648)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:717)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:420)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:78)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:7036)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:254)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1773)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:80)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:130)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:101)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3819)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3871)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3851)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:4105)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:254)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:237)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:394)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:338)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:318)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.org$apache$spark$sql$hive$client$HiveClientImpl$$client(HiveClientImpl.scala:254)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:276)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:221)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:220)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:266)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:356)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:217)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:217)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:217)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:216)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.org$apache$spark$sql$hive$HiveSessionStateBuilder$$externalCatalog(HiveSessionStateBuilder.scala:39)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$1.apply(HiveSessionStateBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$1.apply(HiveSessionStateBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:90)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:90)\n",
      "\tat org.apache.spark.sql.query.analysis.QueryAnalysis$.hiveCatalog(QueryAnalysis.scala:63)\n",
      "\tat org.apache.spark.sql.query.analysis.QueryAnalysis$.getLineageInfo(QueryAnalysis.scala:88)\n",
      "\tat com.cloudera.spark.lineage.NavigatorQueryListener.onSuccess(ClouderaNavigatorListener.scala:60)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager$$anonfun$onSuccess$1$$anonfun$apply$mcV$sp$1.apply(QueryExecutionListener.scala:124)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager$$anonfun$onSuccess$1$$anonfun$apply$mcV$sp$1.apply(QueryExecutionListener.scala:123)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager$$anonfun$org$apache$spark$sql$util$ExecutionListenerManager$$withErrorHandling$1.apply(QueryExecutionListener.scala:145)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager$$anonfun$org$apache$spark$sql$util$ExecutionListenerManager$$withErrorHandling$1.apply(QueryExecutionListener.scala:143)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:392)\n",
      "\tat scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)\n",
      "\tat scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:45)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager.org$apache$spark$sql$util$ExecutionListenerManager$$withErrorHandling(QueryExecutionListener.scala:143)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager$$anonfun$onSuccess$1.apply$mcV$sp(QueryExecutionListener.scala:123)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager$$anonfun$onSuccess$1.apply(QueryExecutionListener.scala:123)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager$$anonfun$onSuccess$1.apply(QueryExecutionListener.scala:123)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager.readLock(QueryExecutionListener.scala:156)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager.onSuccess(QueryExecutionListener.scala:122)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3367)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
      "\tat org.apache.spark.sql.Dataset.show(Dataset.scala:745)\n",
      "\tat org.apache.spark.sql.Dataset.show(Dataset.scala:704)\n",
      "\tat org.apache.spark.sql.Dataset.show(Dataset.scala:713)\n",
      "\tat $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:38)\n",
      "\tat $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:43)\n",
      "\tat $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:45)\n",
      "\tat $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:47)\n",
      "\tat $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:49)\n",
      "\tat $line28.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:51)\n",
      "\tat $line28.$read$$iw$$iw$$iw$$iw.<init>(<console>:53)\n",
      "\tat $line28.$read$$iw$$iw$$iw.<init>(<console>:55)\n",
      "\tat $line28.$read$$iw$$iw.<init>(<console>:57)\n",
      "\tat $line28.$read$$iw.<init>(<console>:59)\n",
      "\tat $line28.$read.<init>(<console>:61)\n",
      "\tat $line28.$read$.<init>(<console>:65)\n",
      "\tat $line28.$read$.<clinit>(<console>)\n",
      "\tat $line28.$eval$.$print$lzycompute(<console>:7)\n",
      "\tat $line28.$eval$.$print(<console>:6)\n",
      "\tat $line28.$eval.$print(<console>)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\n",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\n",
      "\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\n",
      "\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\n",
      "\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n",
      "\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\n",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\n",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreterSpecific$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreterSpecific.scala:385)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreterSpecific$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreterSpecific.scala:380)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
      "\tat scala.Console$.withErr(Console.scala:80)\n",
      "\tat org.apache.toree.global.StreamState$$anonfun$1$$anonfun$apply$1.apply(StreamState.scala:73)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
      "\tat scala.Console$.withOut(Console.scala:53)\n",
      "\tat org.apache.toree.global.StreamState$$anonfun$1.apply(StreamState.scala:72)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
      "\tat scala.Console$.withIn(Console.scala:124)\n",
      "\tat org.apache.toree.global.StreamState$.withStreams(StreamState.scala:71)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreterSpecific$$anonfun$interpretAddTask$1.apply(ScalaInterpreterSpecific.scala:379)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreterSpecific$$anonfun$interpretAddTask$1.apply(ScalaInterpreterSpecific.scala:379)\n",
      "\tat org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$2.run(TaskManager.scala:134)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "+-------+-------+--------+\n",
      "|summary|    UID|_pubyear|\n",
      "+-------+-------+--------+\n",
      "|  count|8517345| 8517345|\n",
      "+-------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "WoS2.describe().filter($\"summary\" === \"count\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java.security.AccessControlException: access denied org.apache.derby.security.SystemPermission( \"engine\", \"usederbyinternals\" )\n",
      "\tat java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)\n",
      "\tat java.security.AccessController.checkPermission(AccessController.java:884)\n",
      "\tat org.apache.derby.iapi.security.SecurityUtil.checkDerbyInternalsPrivilege(Unknown Source)\n",
      "\tat org.apache.derby.iapi.services.monitor.Monitor.startMonitor(Unknown Source)\n",
      "\tat org.apache.derby.iapi.jdbc.JDBCBoot$1.run(Unknown Source)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat org.apache.derby.iapi.jdbc.JDBCBoot.boot(Unknown Source)\n",
      "\tat org.apache.derby.iapi.jdbc.JDBCBoot.boot(Unknown Source)\n",
      "\tat org.apache.derby.jdbc.EmbeddedDriver.boot(Unknown Source)\n",
      "\tat org.apache.derby.jdbc.EmbeddedDriver.<clinit>(Unknown Source)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat java.lang.Class.newInstance(Class.java:442)\n",
      "\tat org.datanucleus.store.rdbms.connectionpool.AbstractConnectionPoolFactory.loadDriver(AbstractConnectionPoolFactory.java:47)\n",
      "\tat org.datanucleus.store.rdbms.connectionpool.DBCPConnectionPoolFactory.createConnectionPool(DBCPConnectionPoolFactory.java:50)\n",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:213)\n",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:117)\n",
      "\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)\n",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:330)\n",
      "\tat org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:203)\n",
      "\tat org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:162)\n",
      "\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:285)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:606)\n",
      "\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n",
      "\tat org.datanucleus.NucleusContextHelper.createStoreManagerForProperties(NucleusContextHelper.java:133)\n",
      "\tat org.datanucleus.PersistenceNucleusContextImpl.initialise(PersistenceNucleusContextImpl.java:420)\n",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:817)\n",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:334)\n",
      "\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:213)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n",
      "\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:610)\n",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:655)\n",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:419)\n",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:356)\n",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:317)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:77)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:137)\n",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)\n",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:688)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:654)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:648)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:717)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:420)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:78)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:84)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:7036)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:254)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1773)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:80)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:130)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:101)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3819)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3871)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3851)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:4105)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:254)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:237)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:394)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:338)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:318)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:294)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.org$apache$spark$sql$hive$client$HiveClientImpl$$client(HiveClientImpl.scala:254)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:276)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:221)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:220)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:266)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:356)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:217)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:217)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:217)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:216)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.org$apache$spark$sql$hive$HiveSessionStateBuilder$$externalCatalog(HiveSessionStateBuilder.scala:39)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$1.apply(HiveSessionStateBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$1.apply(HiveSessionStateBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:90)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:90)\n",
      "\tat org.apache.spark.sql.query.analysis.QueryAnalysis$.hiveCatalog(QueryAnalysis.scala:63)\n",
      "\tat org.apache.spark.sql.query.analysis.QueryAnalysis$.getLineageInfo(QueryAnalysis.scala:88)\n",
      "\tat com.cloudera.spark.lineage.NavigatorQueryListener.onSuccess(ClouderaNavigatorListener.scala:60)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager$$anonfun$onSuccess$1$$anonfun$apply$mcV$sp$1.apply(QueryExecutionListener.scala:124)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager$$anonfun$onSuccess$1$$anonfun$apply$mcV$sp$1.apply(QueryExecutionListener.scala:123)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager$$anonfun$org$apache$spark$sql$util$ExecutionListenerManager$$withErrorHandling$1.apply(QueryExecutionListener.scala:145)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager$$anonfun$org$apache$spark$sql$util$ExecutionListenerManager$$withErrorHandling$1.apply(QueryExecutionListener.scala:143)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:392)\n",
      "\tat scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)\n",
      "\tat scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:45)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager.org$apache$spark$sql$util$ExecutionListenerManager$$withErrorHandling(QueryExecutionListener.scala:143)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager$$anonfun$onSuccess$1.apply$mcV$sp(QueryExecutionListener.scala:123)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager$$anonfun$onSuccess$1.apply(QueryExecutionListener.scala:123)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager$$anonfun$onSuccess$1.apply(QueryExecutionListener.scala:123)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager.readLock(QueryExecutionListener.scala:156)\n",
      "\tat org.apache.spark.sql.util.ExecutionListenerManager.onSuccess(QueryExecutionListener.scala:122)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3367)\n",
      "\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2829)\n",
      "\tat $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:40)\n",
      "\tat $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:46)\n",
      "\tat $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:48)\n",
      "\tat $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:50)\n",
      "\tat $line28.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:52)\n",
      "\tat $line28.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:54)\n",
      "\tat $line28.$read$$iw$$iw$$iw$$iw.<init>(<console>:56)\n",
      "\tat $line28.$read$$iw$$iw$$iw.<init>(<console>:58)\n",
      "\tat $line28.$read$$iw$$iw.<init>(<console>:60)\n",
      "\tat $line28.$read$$iw.<init>(<console>:62)\n",
      "\tat $line28.$read.<init>(<console>:64)\n",
      "\tat $line28.$read$.<init>(<console>:68)\n",
      "\tat $line28.$read$.<clinit>(<console>)\n",
      "\tat $line28.$eval$.$print$lzycompute(<console>:7)\n",
      "\tat $line28.$eval$.$print(<console>:6)\n",
      "\tat $line28.$eval.$print(<console>)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\n",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\n",
      "\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\n",
      "\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\n",
      "\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n",
      "\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\n",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\n",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreterSpecific$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreterSpecific.scala:385)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreterSpecific$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreterSpecific.scala:380)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
      "\tat scala.Console$.withErr(Console.scala:80)\n",
      "\tat org.apache.toree.global.StreamState$$anonfun$1$$anonfun$apply$1.apply(StreamState.scala:73)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
      "\tat scala.Console$.withOut(Console.scala:53)\n",
      "\tat org.apache.toree.global.StreamState$$anonfun$1.apply(StreamState.scala:72)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
      "\tat scala.Console$.withIn(Console.scala:124)\n",
      "\tat org.apache.toree.global.StreamState$.withStreams(StreamState.scala:71)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreterSpecific$$anonfun$interpretAddTask$1.apply(ScalaInterpreterSpecific.scala:379)\n",
      "\tat org.apache.toree.kernel.interpreter.scala.ScalaInterpreterSpecific$$anonfun$interpretAddTask$1.apply(ScalaInterpreterSpecific.scala:379)\n",
      "\tat org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$2.run(TaskManager.scala:134)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.UnsupportedOperationException\n",
       "Message: empty.max\n",
       "StackTrace:   at scala.collection.TraversableOnce$class.max(TraversableOnce.scala:229)\n",
       "  at scala.collection.AbstractTraversable.max(Traversable.scala:104)\n",
       "  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.updateAndGetCompilationStats(CodeGenerator.scala:1354)\n",
       "  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1291)\n",
       "  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1372)\n",
       "  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1369)\n",
       "  at org.spark_project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
       "  at org.spark_project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
       "  at org.spark_project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
       "  at org.spark_project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
       "  at org.spark_project.guava.cache.LocalCache.get(LocalCache.java:4000)\n",
       "  at org.spark_project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)\n",
       "  at org.spark_project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\n",
       "  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1238)\n",
       "  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:204)\n",
       "  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:38)\n",
       "  at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1193)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3382)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
       "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n",
       "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
       "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
       "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
       "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:747)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val yearCount = WoS2.groupBy(\"_pubyear\").count()\n",
    "yearCount.show(yearCount.count.toInt, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 6 in stage 12.0 failed 4 times, most recent failure: Lost task 6.3 in stage 12.0 (TID 1032, iuni9, executor 73): java.lang.RuntimeException: Found duplicate field(s) \"_VALUE\": [_VALUE, _value] in case-insensitive mode\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:315)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:311)\n",
       "\tat scala.Option.map(Option.scala:146)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:311)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:308)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroupFields(ParquetReadSupport.scala:308)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroup(ParquetReadSupport.scala:279)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(ParquetReadSupport.scala:153)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetListType(ParquetReadSupport.scala:230)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(ParquetReadSupport.scala:144)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:318)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:311)\n",
       "\tat scala.Option.map(Option.scala:146)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:311)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:308)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroupFields(ParquetReadSupport.scala:308)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroup(ParquetReadSupport.scala:279)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(ParquetReadSupport.scala:153)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:318)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:311)\n",
       "\tat scala.Option.map(Option.scala:146)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:311)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:308)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroupFields(ParquetReadSupport.scala:308)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroup(ParquetReadSupport.scala:279)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(ParquetReadSupport.scala:153)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:318)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:311)\n",
       "\tat scala.Option.map(Option.scala:146)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:311)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:308)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroupFields(ParquetReadSupport.scala:308)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroup(ParquetReadSupport.scala:279)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(ParquetReadSupport.scala:153)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:318)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:311)\n",
       "\tat scala.Option.map(Option.scala:146)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:311)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:308)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroupFields(ParquetReadSupport.scala:308)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetSchema(ParquetReadSupport.scala:127)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport.init(ParquetReadSupport.scala:77)\n",
       "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:170)\n",
       "\tat org.apache.parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:188)\n",
       "\tat org.apache.parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:145)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:439)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:352)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:126)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:179)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:103)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
       "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)\n",
       "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:315)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:311)\n",
       "\tat scala.Option.map(Option.scala:146)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:311)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:308)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroupFields(ParquetReadSupport.scala:308)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroup(ParquetReadSupport.scala:279)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(ParquetReadSupport.scala:153)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetListType(ParquetReadSupport.scala:230)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(ParquetReadSupport.scala:144)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:318)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:311)\n",
       "\tat scala.Option.map(Option.scala:146)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:311)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:308)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroupFields(ParquetReadSupport.scala:308)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroup(ParquetReadSupport.scala:279)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(ParquetReadSupport.scala:153)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:318)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:311)\n",
       "\tat scala.Option.map(Option.scala:146)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:311)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:308)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroupFields(ParquetReadSupport.scala:308)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroup(ParquetReadSupport.scala:279)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(ParquetReadSupport.scala:153)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:318)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:311)\n",
       "\tat scala.Option.map(Option.scala:146)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:311)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:308)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroupFields(ParquetReadSupport.scala:308)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroup(ParquetReadSupport.scala:279)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(ParquetReadSupport.scala:153)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:318)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:311)\n",
       "\tat scala.Option.map(Option.scala:146)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:311)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:308)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n",
       "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "\tat org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroupFields(ParquetReadSupport.scala:308)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetSchema(ParquetReadSupport.scala:127)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport.init(ParquetReadSupport.scala:77)\n",
       "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:170)\n",
       "\tat org.apache.parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:188)\n",
       "\tat org.apache.parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:145)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:439)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:352)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:126)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:179)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:103)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
       "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)\n",
       "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2102)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2121)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2146)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
       "  at org.apache.spark.sql.execution.stat.StatFunctions$.aggResult$lzycompute$1(StatFunctions.scala:273)\n",
       "  at org.apache.spark.sql.execution.stat.StatFunctions$.org$apache$spark$sql$execution$stat$StatFunctions$$aggResult$1(StatFunctions.scala:273)\n",
       "  at org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$summary$2.apply$mcVI$sp(StatFunctions.scala:286)\n",
       "  at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)\n",
       "  at org.apache.spark.sql.execution.stat.StatFunctions$.summary(StatFunctions.scala:285)\n",
       "  at org.apache.spark.sql.Dataset.summary(Dataset.scala:2533)\n",
       "  at org.apache.spark.sql.Dataset.describe(Dataset.scala:2472)\n",
       "  ... 46 elided\n",
       "Caused by: java.lang.RuntimeException: Found duplicate field(s) \"_VALUE\": [_VALUE, _value] in case-insensitive mode\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:315)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:311)\n",
       "  at scala.Option.map(Option.scala:146)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:311)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:308)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "  at org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroupFields(ParquetReadSupport.scala:308)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroup(ParquetReadSupport.scala:279)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(ParquetReadSupport.scala:153)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetListType(ParquetReadSupport.scala:230)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(ParquetReadSupport.scala:144)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:318)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:311)\n",
       "  at scala.Option.map(Option.scala:146)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:311)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:308)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "  at org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroupFields(ParquetReadSupport.scala:308)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroup(ParquetReadSupport.scala:279)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(ParquetReadSupport.scala:153)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:318)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:311)\n",
       "  at scala.Option.map(Option.scala:146)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:311)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:308)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "  at org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroupFields(ParquetReadSupport.scala:308)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroup(ParquetReadSupport.scala:279)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(ParquetReadSupport.scala:153)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:318)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:311)\n",
       "  at scala.Option.map(Option.scala:146)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:311)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:308)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "  at org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroupFields(ParquetReadSupport.scala:308)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroup(ParquetReadSupport.scala:279)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(ParquetReadSupport.scala:153)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:318)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2$$anonfun$apply$3.apply(ParquetReadSupport.scala:311)\n",
       "  at scala.Option.map(Option.scala:146)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:311)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$$anonfun$clipParquetGroupFields$2.apply(ParquetReadSupport.scala:308)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n",
       "  at org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetGroupFields(ParquetReadSupport.scala:308)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport$.clipParquetSchema(ParquetReadSupport.scala:127)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport.init(ParquetReadSupport.scala:77)\n",
       "  at org.apache.parquet.hadoop.InternalParquetRecordReader.initialize(InternalParquetRecordReader.java:170)\n",
       "  at org.apache.parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:188)\n",
       "  at org.apache.parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:145)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:439)\n",
       "  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1.apply(ParquetFileFormat.scala:352)\n",
       "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:126)\n",
       "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:179)\n",
       "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:103)\n",
       "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
       "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
       "  at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)\n",
       "  at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$12.apply(RDD.scala:823)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
       "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n",
       "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WoS2.describe().filter($\"summary\" === \"count\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UID: string (nullable = true)\n",
      " |-- identifier: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _type: string (nullable = true)\n",
      " |    |    |-- _value: string (nullable = true)\n",
      " |-- p: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- doctype: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- _pubyear: long (nullable = true)\n",
      " |-- display_name: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "esci2 = [UID: string, identifier: array<struct<_VALUE:string,_type:string,_value:string>> ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[UID: string, identifier: array<struct<_VALUE:string,_type:string,_value:string>> ... 4 more fields]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val esci2 = esci.select(\"UID\",\"dynamic_data.cluster_related.identifiers.identifier\",\"static_data.abstracts.abstract.abstract_text.p\",\"static_data.summary.doctypes.doctype\",\n",
    "                       \"static_data.summary.pub_info._pubyear\",\"static_data.summary.names.name.display_name\")\n",
    "esci2.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 81:=================================================>        (6 + 1) / 7]+--------+------+\n",
      "|_pubyear|count |\n",
      "+--------+------+\n",
      "|2016    |291622|\n",
      "|2018    |25    |\n",
      "|2017    |307040|\n",
      "+--------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "esciCount = [_pubyear: bigint, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_pubyear: bigint, count: bigint]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val esciCount = esci2.groupBy(\"_pubyear\").count()\n",
    "esciCount.show(yearCount.count.toInt, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Some(http://iuni2:4040)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
