{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['field1', '', 'field3', 'field4']\n",
      "field1,,field3,field4\n",
      "\n",
      "('a', 1)\n",
      "{\"IndexLength\":34,\n",
      "\"InvertedIndex\":{\"Erfolgreiche\":[0],\"Arbeitsteilung\":[1],\"im\":[2],\"beruflichen\":[3],\"Kontext\":[4],\"setzt\":[5],\"voraus,\":[6],\"dass\":[7,25],\"die\":[8,17],\"Beteiligten\":[9],\"eine\":[10],\"angemessene\":[11],\"Art\":[12],\"und\":[13,24],\"Weise\":[14],\"finden,\":[15],\"uber\":[16],\"Aufteilung\":[18],\"der\":[19,30],\"Arbeit\":[20,31],\"zu\":[21],\"kommunizieren\":[22],\"â€“\":[23],\"sie\":[26],\"ihren\":[27],\"jeweiligen\":[28],\"Teil\":[29],\"tatsachlich\":[32],\"erledigen.\":[33]}}\n",
      "\n",
      "['a', 1, 'b', 'c']\n",
      "\"aaaa\"\n",
      "ConferenceSeries.csv\n",
      "PaperAbstract.csv\n",
      "FieldsOfStudy.csv\n",
      "ConferenceInstance.csv\n",
      "Author.csv\n",
      "Affiliation.csv\n",
      "Journal.csv\n",
      "Paper.csv\n",
      "relation\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Empty\n"
     ]
    }
   ],
   "source": [
    "test = \"field1\\t\\tfield3\\tfield4\"\n",
    "\n",
    "tokens = test.split('\\t')\n",
    "\n",
    "print(tokens)\n",
    "print(','.join(tokens))\n",
    "\n",
    "a = ('a', 1)\n",
    "print()\n",
    "print(a)\n",
    "\n",
    "jsonStr = '''{\"IndexLength\":34,\n",
    "\"InvertedIndex\":{\"Erfolgreiche\":[0],\"Arbeitsteilung\":[1],\"im\":[2],\"beruflichen\":[3],\"Kontext\":[4],\"setzt\":[5],\"voraus,\":[6],\"dass\":[7,25],\"die\":[8,17],\"Beteiligten\":[9],\"eine\":[10],\"angemessene\":[11],\"Art\":[12],\"und\":[13,24],\"Weise\":[14],\"finden,\":[15],\"uber\":[16],\"Aufteilung\":[18],\"der\":[19,30],\"Arbeit\":[20,31],\"zu\":[21],\"kommunizieren\":[22],\"\\u2013\":[23],\"sie\":[26],\"ihren\":[27],\"jeweiligen\":[28],\"Teil\":[29],\"tatsachlich\":[32],\"erledigen.\":[33]}}\n",
    "'''\n",
    "print(jsonStr)\n",
    "\n",
    "a = ['a', 1]\n",
    "print(a + ['b', 'c'])\n",
    "\n",
    "string = \"aaaa\"\n",
    "\n",
    "print('\"' + string + '\"')\n",
    "\n",
    "directory = outputRoot\n",
    "outDir = '/N/dc2/projects/rds/gruan/workspace/CADRE/data/namespaced/nodes'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    print(filename)\n",
    "    \n",
    "for idx in range(5):\n",
    "    print(idx)\n",
    "    \n",
    "if not '':\n",
    "    print(\"Empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, time, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAG_Data_Root = \"/N/dc2/projects/rds/gruan/workspace/CADRE/data/mag\"\n",
    "\n",
    "outputRoot = \"/N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed\"\n",
    "\n",
    "relationOutputRoot = \"/N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation\"\n",
    "\n",
    "inputDelimiter = '\\t'\n",
    "outputDelimiter = '\\t'\n",
    "\n",
    "Affilication_Table_Header = ['AffiliationId', 'Rank', 'NormalizedName', 'DisplayName', 'GridId', 'OfficialPage', 'WikiPage',\n",
    "                       'PaperCount', 'CitationCount', 'CreatedDate']\n",
    "\n",
    "Author_Table_Header = ['AuthorId', 'Rank', 'NormalizedName', 'DisplayName', 'LastKnownAffiliationId', \n",
    "                       'PaperCount', 'CitationCount', 'CreatedDate']\n",
    "\n",
    "ConferenceInstance_Table_Header = ['ConferenceInstanceId', 'NormalizedName', 'DisplayName', 'ConferenceSeriesId',\n",
    "                                  'Location', 'OfficialUrl', 'StartDate', 'EndDate', 'AbstractRegistrationDate', \n",
    "                                  'SubmissionDeadlineDate', 'NotificationDueDate', 'FinalVersionDueDate',\n",
    "                                  'PaperCount', 'CitationCount', 'CreatedDate']\n",
    "\n",
    "Journal_Table_Header = ['JournalId', 'Rank', 'NormalizedName', 'DisplayName', 'Issn', 'Publisher', \n",
    "                        'Webpage', 'PaperCount', 'CitationCount', 'CreatedDate']\n",
    "\n",
    "ConferenceSeries_Table_Header = ['ConferenceSeriesId', 'Rank', 'NormalizedName', 'DisplayName', 'PaperCount',\n",
    "                                'CitationCount', 'CreatedDate']\n",
    "\n",
    "FieldOfStudy_Table_Header = ['FieldOfStudyId', 'Rank', 'NormalizedName', 'DisplayName', 'MainType', 'Level',\n",
    "                            'PaperCount', 'CitationCount', 'CreatedDate']\n",
    "\n",
    "PaperAbstract_Table_Header = ['PaperId', 'Abstract']\n",
    "\n",
    "PaperAuthorAffiliation_Table_Header = ['PaperId', 'AuthorId', 'AffiliationId', 'AuthorSequenceNumber',\n",
    "                                      'OriginalAuthor', 'OriginalAffiliation']\n",
    "\n",
    "PaperFieldsOfStudy_Table_Header = ['PaperId', 'FieldOfStudyId', 'Score']\n",
    "\n",
    "FieldOfStudyChildren_Table_Header = ['FieldOfStudyId', 'ChildFieldOfStudyId']\n",
    "\n",
    "RelatedFieldOfStudy_Table_Header = ['FieldOfStudyId1', 'Type1', 'FieldOfStudyId2', 'Type2', 'Rank']\n",
    "\n",
    "PaperReferences_Table_Header = ['PaperId', 'PaperReferenceId']\n",
    "\n",
    "PaperRecommendations_Table_Header = ['PaperId', 'RecommendedPaperId', 'Score']\n",
    "\n",
    "Paper_Table_Header = ['PaperId', 'Rank', 'Doi', 'DocType', 'PaperTitle', 'OriginalTitle', \n",
    "                     'BookTitle', 'Year', 'Date', 'Publisher', 'JournalId', 'ConferenceSeriesId',\n",
    "                     'ConferenceInstanceId', 'Volume', 'Issue', 'FirstPage', 'LastPage',\n",
    "                     'ReferenceCount', 'CitationCount', 'EstimatedCitation', 'OriginalVenue', 'CreatedDate']\n",
    "\n",
    "Node_ID_Transform_Table = {\"PaperAbstract.csv\" : \"pa:\", \"FieldsOfStudy.csv\" : \"f:\", \"ConferenceSeries.csv\" : \"cs:\",\n",
    "                          \"Author.csv\" : \"au:\", \"ConferenceInstance.csv\" : \"ci:\", \n",
    "                          \"Journal.csv\" : \"j:\", \"Affiliation.csv\" : \"af:\", \"Paper.csv\" : \"p:\"}\n",
    "\n",
    "Relation_ID_Transform_Table = {\"AffiliatedWith.csv\" : {0 : \"au:\", 1 : \"af:\"},\n",
    "                               \"AuthorOf.csv\" : {0 : \"au:\", 1 : \"p:\"},\n",
    "                              \"BelongsTo.csv\" : {0 : \"p:\", 1 : \"f:\"},\n",
    "                               \"ChildOf.csv\" : {0 : \"f:\", 1 : \"f:\"},\n",
    "                               \"InstanceOf.csv\" : {0 : \"ci:\", 1 : \"cs:\"},\n",
    "                               \"PresentedAt.csv\" : {0 : \"p:\", 1 : \"ci:\"},\n",
    "                               \"PublishedIn.csv\" : {0 : \"p:\", 1 : \"j:\"},\n",
    "                               \"Recommendations.csv\" : {0 : \"p:\", 1 : \"p:\"},\n",
    "                               \"References.csv\" : {0 : \"p:\", 1 : \"p:\"},\n",
    "                               \"RelatedTo.csv\" : {0 : \"f:\", 1 : \"f:\"}\n",
    "                              }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text2CSV(inputFilePath, outputFilePath, inputDelimiter, outputDelimiter, \n",
    "             inputCSVHeader, outputCSVHeader, transformFunc):\n",
    "    \n",
    "    # write header\n",
    "    with open(outputFilePath, \"w\", encoding = 'utf-8') as f:\n",
    "        f.write(outputDelimiter.join(outputCSVHeader) + '\\n')\n",
    "        \n",
    "    totalLines = 0\n",
    "    brokenLines = 0\n",
    "    problematicLines = 0\n",
    "    numInFields = len(inputCSVHeader)\n",
    "    numOutFields = len(outputCSVHeader)\n",
    "    \n",
    "    # write actual content\n",
    "    with open(outputFilePath, \"a\", encoding = 'utf-8') as outputFile:\n",
    "        \n",
    "        with open(inputFilePath, \"r\", encoding = 'utf-8') as inputFile: \n",
    "            for line in inputFile:\n",
    "                totalLines += 1\n",
    "                \n",
    "                # remove leading/trailing whitespace chars\n",
    "                fields = line.strip().split(inputDelimiter)\n",
    "                \n",
    "                if len(fields) != numInFields:\n",
    "                    brokenLines += 1\n",
    "                    continue\n",
    "                    \n",
    "                transformedLine = transformFunc(fields, outputDelimiter)\n",
    "                \n",
    "                # verify it is OK using the outputDelimiter\n",
    "                if len(transformedLine.split(outputDelimiter)) != numOutFields:\n",
    "                    problematicLines += 1\n",
    "                    \n",
    "                outputFile.write(transformedLine + '\\n')\n",
    "                  \n",
    "    print('Finished processing {}, bad lines: {}/{}/{}'.format(inputFilePath, \n",
    "                                                               brokenLines, problematicLines, totalLines))\n",
    "\n",
    "def joinByDelimiter(fields, delimiter):\n",
    "    \n",
    "    return delimiter.join(fields)\n",
    "\n",
    "def invertedIdx2Abstract(fields, delimiter):\n",
    "    \n",
    "    invertedIdx = json.loads(fields[1])\n",
    "    \n",
    "    abstract = [''] * invertedIdx['IndexLength']\n",
    "    \n",
    "    for key, value in invertedIdx['InvertedIndex'].items():\n",
    "        \n",
    "        if key.isspace():\n",
    "            continue\n",
    "            \n",
    "        for offset in value:\n",
    "            abstract[offset] = key\n",
    "            \n",
    "    abstract = ' '.join(abstract)\n",
    "    \n",
    "    # add quoatation in case it contains delimiter \n",
    "    abstract = '\"' + abstract + '\"'\n",
    "    return delimiter.join([fields[0], abstract])\n",
    "\n",
    "def parsePaperAuthorAffiliation(inputFilePath, inputHeader, \n",
    "                                affiliatedWithOutFilePath, authorOfOutFilePath,\n",
    "                               inputDelimiter, outputDelimiter,\n",
    "                               affiliatedWithHeader, authorOfHeader):\n",
    "    authorDic = {}\n",
    "    totalLines = 0\n",
    "    \n",
    "    with open(inputFilePath, \"r\", encoding = 'utf-8') as inputFile: \n",
    "        for line in inputFile:\n",
    "            totalLines += 1\n",
    "            fields = line.strip().split(inputDelimiter)\n",
    "\n",
    "            paperId = fields[0]\n",
    "            authorId = fields[1]\n",
    "            affiliationId = fields[2]\n",
    "\n",
    "            if authorId not in authorDic:\n",
    "                authorDic[authorId] = {'affiliations' : [], 'papers' : []}\n",
    "                \n",
    "            authorDic[authorId]['affiliations'].append(affiliationId)\n",
    "            authorDic[authorId]['papers'].append(paperId)\n",
    "            \n",
    "    print('Finished processing {}, procssed lines: {}'.format(inputFilePath, totalLines))\n",
    "    \n",
    "    ## generate AffiliatedWith and AuthorOf relationship\n",
    "    \n",
    "    # write header\n",
    "    with open(affiliatedWithOutFilePath, \"w\", encoding = 'utf-8') as f:\n",
    "        f.write(outputDelimiter.join(affiliatedWithHeader) + '\\n')\n",
    "        \n",
    "    # write header\n",
    "    with open(authorOfOutFilePath, \"w\", encoding = 'utf-8') as f:\n",
    "        f.write(outputDelimiter.join(authorOfHeader) + '\\n')\n",
    "        \n",
    "    with open(affiliatedWithOutFilePath, \"a\", encoding = 'utf-8') as affiliatedWithFile:   \n",
    "        \n",
    "        with open(authorOfOutFilePath, \"a\", encoding = 'utf-8') as authorOfFile:  \n",
    "        \n",
    "            for key, value in authorDic.items():\n",
    "            \n",
    "                for v in value['affiliations']:\n",
    "                    affiliatedWithFile.write(outputDelimiter.join([key, v]) + '\\n')\n",
    "                    \n",
    "                for v in value['papers']:\n",
    "                    authorOfFile.write(outputDelimiter.join([key, v]) + '\\n') \n",
    "\n",
    "                    \n",
    "def parseConfInstanceOfRelation(fields, delimiter):\n",
    "    \n",
    "    instanceId = fields[0]\n",
    "    seriesId = fields[3]\n",
    "    \n",
    "    return delimiter.join([instanceId, seriesId])\n",
    "\n",
    "def parseChildOfRelation(fields, delimiter):\n",
    "    \n",
    "    fieldId = fields[0]\n",
    "    childFieldId = fields[1]\n",
    "    \n",
    "    return delimiter.join([childFieldId, fieldId])\n",
    "\n",
    "def parseRelatedTo(fields, delimiter):\n",
    "    \n",
    "    fieldId1 = fields[0]\n",
    "    fieldId2 = fields[2]\n",
    "    rank = fields[4]\n",
    "    \n",
    "    return delimiter.join([fieldId1, fieldId2, rank])\n",
    "\n",
    "def parsePublishedIn(fields, delimiter):\n",
    "    \n",
    "    paperId = fields[0]\n",
    "    journalId = fields[10]\n",
    "    \n",
    "    return delimiter.join([paperId, journalId])\n",
    "\n",
    "def parsePresentedAt(fields, delimiter):\n",
    "    \n",
    "    paperId = fields[0]\n",
    "    confInsId = fields[12]\n",
    "    \n",
    "    return delimiter.join([paperId, confInsId])\n",
    "\n",
    "def genPaperNode(languageInputFilePath, urlInputFilePath, papersInputFilePath, outputFilePath, \n",
    "                 inputDelimiter, outputDelimiter, inputCSVHeader, outputCSVHeader):\n",
    "    \n",
    "    paperLangCodeDict = {}\n",
    "    paperUrlDict = {}\n",
    "    \n",
    "    totalLines = 0\n",
    "    \n",
    "    with open(languageInputFilePath, \"r\", encoding = 'utf-8') as f: \n",
    "        \n",
    "        for line in f:\n",
    "            totalLines += 1;\n",
    "            fields = line.strip().split(inputDelimiter)\n",
    "\n",
    "            paperId = fields[0]\n",
    "            langCode = fields[1]\n",
    "            \n",
    "            if paperId not in paperLangCodeDict:\n",
    "                paperLangCodeDict[paperId] = []\n",
    "                \n",
    "            paperLangCodeDict[paperId].append(langCode)\n",
    "     \n",
    "    print('Finished processing {}, procssed lines: {}'.format(languageInputFilePath, totalLines))\n",
    "    \n",
    "    totalLines = 0\n",
    "    with open(urlInputFilePath, \"r\", encoding = 'utf-8') as f: \n",
    "        \n",
    "        for line in f:\n",
    "            totalLines += 1;\n",
    "            fields = line.strip().split(inputDelimiter)\n",
    "\n",
    "            paperId = fields[0]\n",
    "            url = fields[2]\n",
    "            \n",
    "            if paperId not in paperUrlDict:\n",
    "                paperUrlDict[paperId] = []\n",
    "                \n",
    "            paperUrlDict[paperId].append(url) \n",
    "            \n",
    "    print('Finished processing {}, procssed lines: {}'.format(urlInputFilePath, totalLines))\n",
    "    \n",
    "    # write header\n",
    "    with open(outputFilePath, \"w\", encoding = 'utf-8') as f:\n",
    "        f.write(outputDelimiter.join(outputCSVHeader) + '\\n')\n",
    "        \n",
    "    totalLines = 0\n",
    "    brokenLines = 0\n",
    "    problematicLines = 0\n",
    "    numInFields = len(inputCSVHeader)\n",
    "    numOutFields = len(outputCSVHeader)\n",
    "    \n",
    "    # write actual content\n",
    "    with open(outputFilePath, \"a\", encoding = 'utf-8') as outputFile:\n",
    "        with open(papersInputFilePath, \"r\", encoding = 'utf-8') as f: \n",
    "            for line in f:\n",
    "                totalLines += 1\n",
    "                fields = line.strip().split(inputDelimiter)\n",
    "                \n",
    "                if len(fields) != numInFields:\n",
    "                    brokenLines += 1\n",
    "                    continue\n",
    "                    \n",
    "                paperId = fields[0]\n",
    "                \n",
    "                if paperId in paperLangCodeDict:\n",
    "                    langCodes = '\"' + ','.join(paperLangCodeDict[paperId]) + '\"'\n",
    "                else:\n",
    "                    langCodes = '\"\"'\n",
    "                    \n",
    "                if paperId in paperUrlDict:\n",
    "                    urls = '\"' + ','.join(paperUrlDict[paperId]) + '\"'\n",
    "                else:\n",
    "                    urls = '\"\"'\n",
    "                \n",
    "                fields.append(langCodes)\n",
    "                fields.append(urls)\n",
    "                \n",
    "                transformedLine = outputDelimiter.join(fields)\n",
    "                \n",
    "                # verify it is OK using the outputDelimiter\n",
    "                if len(transformedLine.split(outputDelimiter)) != numOutFields:\n",
    "                    problematicLines += 1\n",
    "                    \n",
    "                outputFile.write(transformedLine + '\\n')\n",
    "                \n",
    "    print('Finished processing {}, bad lines: {}/{}/{}'.format(papersInputFilePath, \n",
    "                                                               brokenLines, problematicLines, totalLines))\n",
    "    \n",
    "# assume id is the first field of each line\n",
    "def prependNameSpace(inputFilePath, outputFilePath, namespace):\n",
    "    \n",
    "    lineCnt = 0\n",
    "    with open(outputFilePath, \"w\", encoding = 'utf-8') as outputFile:\n",
    "        with open(inputFilePath, \"r\", encoding = 'utf-8') as f: \n",
    "            for line in f:\n",
    "                lineCnt += 1\n",
    "                \n",
    "                if lineCnt == 1:# header\n",
    "                    outputFile.write(line.strip() + '\\n')\n",
    "                    continue\n",
    "                    \n",
    "                outputFile.write(namespace + line.strip() + '\\n')\n",
    "                \n",
    "    print('Finished processing {}, processed lines {}'.format(inputFilePath, lineCnt))           \n",
    "                           \n",
    "def transformNameSpace(inputFilePath, outputFilePath, delimiter, mapping):\n",
    "    \n",
    "    lineCnt = 0\n",
    "    with open(outputFilePath, \"w\", encoding = 'utf-8') as outputFile:\n",
    "        with open(inputFilePath, \"r\", encoding = 'utf-8') as f: \n",
    "            for line in f:\n",
    "                lineCnt += 1\n",
    "                \n",
    "                if lineCnt == 1:# header\n",
    "                    outputFile.write(line.strip() + '\\n')\n",
    "                    continue\n",
    "                    \n",
    "                fields = line.strip().split(delimiter)\n",
    "                \n",
    "                for idx in range(len(fields)):\n",
    "                    \n",
    "                    if idx in mapping:\n",
    "                        fields[idx] = mapping[idx] + fields[idx]\n",
    "                \n",
    "                outLine = delimiter.join(fields)\n",
    "                outputFile.write(outLine + '\\n')\n",
    "                \n",
    "    print('Finished processing {}, processed lines {}'.format(inputFilePath, lineCnt))  \n",
    "    \n",
    "def cleanRelation(inputFilePath, outputFilePath, delimiter):\n",
    "    \n",
    "    lineCnt = 0\n",
    "    badLines = 0\n",
    "    numFields = 0\n",
    "    \n",
    "    with open(outputFilePath, \"w\", encoding = 'utf-8') as outputFile:\n",
    "        with open(inputFilePath, \"r\", encoding = 'utf-8') as f: \n",
    "            for line in f:\n",
    "                lineCnt += 1\n",
    "                \n",
    "                line = line.strip()\n",
    "                \n",
    "                if lineCnt == 1:# header\n",
    "                    numFields = len(line.split(delimiter))\n",
    "                    outputFile.write(line + '\\n')\n",
    "                    continue\n",
    "                    \n",
    "                fields = line.split(delimiter)\n",
    "                \n",
    "                if len(fields) != numFields:\n",
    "                    badLines += 1\n",
    "                    continue\n",
    "                    \n",
    "                isBadLine = False\n",
    "                for field in fields:\n",
    "                    if not field or field.isspace():\n",
    "                        isBadLine = True;\n",
    "                        break\n",
    "                        \n",
    "                if isBadLine:\n",
    "                    badLines += 1\n",
    "                    continue\n",
    "                \n",
    "                outputFile.write(line + '\\n')\n",
    "                \n",
    "    print('Finished processing {}, bad lines {}/{}'.format(inputFilePath, badLines, lineCnt))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/BelongsTo.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/BelongsTo.csv, bad lines 0/1052519199\n",
      "Elapsed time in min 42.52143074671427\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/AffiliatedWith.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/AffiliatedWith.csv, bad lines 403994029/560971808\n",
      "Elapsed time in min 12.354348945617676\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/RelatedTo.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/RelatedTo.csv, bad lines 0/128638\n",
      "Elapsed time in min 0.006583809852600098\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/PresentedAt.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/PresentedAt.csv, bad lines 210764479/212209776\n",
      "Elapsed time in min 3.1819719473520913\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/ChildOf.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/ChildOf.csv, bad lines 0/349013\n",
      "Elapsed time in min 0.018130481243133545\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/InstanceOf.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/InstanceOf.csv, bad lines 0/15738\n",
      "Elapsed time in min 0.0014921069145202638\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/PublishedIn.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/PublishedIn.csv, bad lines 130579446/212209776\n",
      "Elapsed time in min 5.693361882368723\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/Recommendations.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/Recommendations.csv, bad lines 0/2535427127\n",
      "Elapsed time in min 117.30624980131785\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/References.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/References.csv, bad lines 0/1399752646\n",
      "Elapsed time in min 50.61759095986684\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/AuthorOf.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/AuthorOf.csv, bad lines 0/560971808\n",
      "Elapsed time in min 19.264869836966195\n"
     ]
    }
   ],
   "source": [
    "directory = os.path.join(outputRoot, 'relation')\n",
    "outDir = os.path.join(outputRoot, 'relation', 'cleaned')\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "        \n",
    "    inputFilePath = os.path.join(directory, filename) \n",
    "    \n",
    "    if os.path.isdir(inputFilePath):\n",
    "        continue\n",
    "        \n",
    "    outputFilePath = os.path.join(outDir, filename)\n",
    "    \n",
    "    print('Start processing input file: {}'.format(inputFilePath))\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    cleanRelation(inputFilePath, outputFilePath, inputDelimiter)\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    runtime = (end - start) / 60.0\n",
    "    print('Elapsed time in min {}'.format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/BelongsTo.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/BelongsTo.csv, processed lines 1052519199\n",
      "Elapsed time in min 63.702239056428276\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/AffiliatedWith.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/AffiliatedWith.csv, processed lines 156977779\n",
      "Elapsed time in min 8.504526003201802\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/RelatedTo.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/RelatedTo.csv, processed lines 128638\n",
      "Elapsed time in min 0.009409769376118978\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/PresentedAt.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/PresentedAt.csv, processed lines 1445297\n",
      "Elapsed time in min 0.07843863566716512\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/ChildOf.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/ChildOf.csv, processed lines 349013\n",
      "Elapsed time in min 0.01985477606455485\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/InstanceOf.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/InstanceOf.csv, processed lines 15738\n",
      "Elapsed time in min 0.002539356549580892\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/PublishedIn.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/PublishedIn.csv, processed lines 81630330\n",
      "Elapsed time in min 4.539845482508341\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/Recommendations.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/Recommendations.csv, processed lines 2535427127\n",
      "Elapsed time in min 169.07173307339352\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/References.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/References.csv, processed lines 1399752646\n",
      "Elapsed time in min 76.7045000553131\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/AuthorOf.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/relation/cleaned/AuthorOf.csv, processed lines 560971808\n",
      "Elapsed time in min 30.196927853425343\n"
     ]
    }
   ],
   "source": [
    "directory = os.path.join(outputRoot, 'relation', 'cleaned')\n",
    "outDir = '/N/dc2/projects/rds/gruan/workspace/CADRE/data/namespaced/relationships'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    \n",
    "    if filename not in Relation_ID_Transform_Table:\n",
    "        continue\n",
    "        \n",
    "    inputFilePath = os.path.join(directory, filename)    \n",
    "    outputFilePath = os.path.join(outDir, filename)\n",
    "    \n",
    "    print('Start processing input file: {}'.format(inputFilePath))\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    transformNameSpace(inputFilePath, outputFilePath, inputDelimiter, Relation_ID_Transform_Table[filename])\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    runtime = (end - start) / 60.0\n",
    "    print('Elapsed time in min {}'.format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/ConferenceSeries.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/ConferenceSeries.csv, processed lines 4352\n",
      "Elapsed time in min 0.00024244387944539388\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/PaperAbstract.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/PaperAbstract.csv, processed lines 143244152\n",
      "Elapsed time in min 38.51862007379532\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/FieldsOfStudy.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/FieldsOfStudy.csv, processed lines 229848\n",
      "Elapsed time in min 0.011437857151031494\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/ConferenceInstance.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/ConferenceInstance.csv, processed lines 15738\n",
      "Elapsed time in min 0.0019686619440714517\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/Author.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/Author.csv, processed lines 255991187\n",
      "Elapsed time in min 7.146963103612264\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/Affiliation.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/Affiliation.csv, processed lines 25455\n",
      "Elapsed time in min 0.002112225691477458\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/Journal.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/Journal.csv, processed lines 48669\n",
      "Elapsed time in min 0.0026058514912923178\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/Paper.csv\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/transformed/Paper.csv, processed lines 212209776\n",
      "Elapsed time in min 21.13901332219442\n"
     ]
    }
   ],
   "source": [
    "directory = outputRoot\n",
    "outDir = '/N/dc2/projects/rds/gruan/workspace/CADRE/data/namespaced/nodes'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    \n",
    "    if filename not in Node_ID_Transform_Table:\n",
    "        continue\n",
    "        \n",
    "    inputFilePath = os.path.join(directory, filename)    \n",
    "    outputFilePath = os.path.join(outDir, filename)\n",
    "    \n",
    "    print('Start processing input file: {}'.format(inputFilePath))\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    prependNameSpace(inputFilePath, outputFilePath, Node_ID_Transform_Table[filename])\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    runtime = (end - start) / 60.0\n",
    "    print('Elapsed time in min {}'.format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate paper node\n",
    "\n",
    "languageInputFile = 'PaperLanguages.txt'\n",
    "urlInputFile = 'PaperUrls.txt'\n",
    "papersInputFile = 'Papers.txt'\n",
    "outputFile = 'Paper.csv'\n",
    "\n",
    "languageInputFilePath = os.path.join(MAG_Data_Root, languageInputFile)\n",
    "urlInputFilePath = os.path.join(MAG_Data_Root, urlInputFile)\n",
    "papersInputFilePath = os.path.join(MAG_Data_Root, papersInputFile)\n",
    "\n",
    "outputFilePath = os.path.join(outputRoot, outputFile)\n",
    "inputCSVHeader = Paper_Table_Header\n",
    "outputCSVHeader = Paper_Table_Header + ['LanguageCodes', 'Urls']\n",
    "\n",
    "print('Start processing input file: {}'.format(papersInputFilePath))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "genPaperNode(languageInputFilePath, urlInputFilePath, papersInputFilePath, outputFilePath, \n",
    "                 inputDelimiter, outputDelimiter, inputCSVHeader, outputCSVHeader)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "runtime = (end - start) / 60.0\n",
    "print('Elapsed time in min {}'.format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/PaperFieldsOfStudy.txt\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/PaperFieldsOfStudy.txt, bad lines: 0/0/1052519198\n",
      "Elapsed time in min 49.893501762549086\n"
     ]
    }
   ],
   "source": [
    "# generate belongsTo relathonship\n",
    "\n",
    "inputFile = 'PaperFieldsOfStudy.txt'\n",
    "outputFile = 'BelongsTo.csv'\n",
    "\n",
    "inputFilePath = os.path.join(MAG_Data_Root, inputFile)\n",
    "outputFilePath = os.path.join(relationOutputRoot, outputFile)\n",
    "inputCSVHeader = PaperFieldsOfStudy_Table_Header\n",
    "outputCSVHeader = ['PaperId', 'FieldOfStudyId', 'Weight']\n",
    "\n",
    "print('Start processing input file: {}'.format(inputFilePath))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "text2CSV(inputFilePath, outputFilePath, inputDelimiter, outputDelimiter, \n",
    "         inputCSVHeader, outputCSVHeader, joinByDelimiter)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "runtime = (end - start) / 60.0\n",
    "print('Elapsed time in min {}'.format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/ConferenceInstances.txt\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/ConferenceInstances.txt, bad lines: 0/0/15737\n",
      "Elapsed time in min 0.001520991325378418\n"
     ]
    }
   ],
   "source": [
    "# generate instanceOf relathonship\n",
    "\n",
    "inputFile = 'ConferenceInstances.txt'\n",
    "outputFile = 'InstanceOf.csv'\n",
    "\n",
    "inputFilePath = os.path.join(MAG_Data_Root, inputFile)\n",
    "outputFilePath = os.path.join(relationOutputRoot, outputFile)\n",
    "inputCSVHeader = ConferenceInstance_Table_Header\n",
    "outputCSVHeader = ['ConferenceInstanceId', 'ConferenceSeriesId']\n",
    "\n",
    "print('Start processing input file: {}'.format(inputFilePath))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "text2CSV(inputFilePath, outputFilePath, inputDelimiter, outputDelimiter, \n",
    "         inputCSVHeader, outputCSVHeader, parseConfInstanceOfRelation)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "runtime = (end - start) / 60.0\n",
    "print('Elapsed time in min {}'.format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/FieldOfStudyChildren.txt\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/FieldOfStudyChildren.txt, bad lines: 0/0/349012\n",
      "Elapsed time in min 0.020771944522857667\n"
     ]
    }
   ],
   "source": [
    "# generate childOf relathonship\n",
    "\n",
    "inputFile = 'FieldOfStudyChildren.txt'\n",
    "outputFile = 'ChildOf.csv'\n",
    "\n",
    "inputFilePath = os.path.join(MAG_Data_Root, inputFile)\n",
    "outputFilePath = os.path.join(relationOutputRoot, outputFile)\n",
    "inputCSVHeader = FieldOfStudyChildren_Table_Header\n",
    "outputCSVHeader = ['FieldOfStudyId', 'FieldOfStudyId']\n",
    "\n",
    "print('Start processing input file: {}'.format(inputFilePath))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "text2CSV(inputFilePath, outputFilePath, inputDelimiter, outputDelimiter, \n",
    "         inputCSVHeader, outputCSVHeader, parseChildOfRelation)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "runtime = (end - start) / 60.0\n",
    "print('Elapsed time in min {}'.format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/RelatedFieldOfStudy.txt\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/RelatedFieldOfStudy.txt, bad lines: 0/0/128637\n",
      "Elapsed time in min 0.011946598688761393\n"
     ]
    }
   ],
   "source": [
    "# generate relatedTo relathonship\n",
    "\n",
    "inputFile = 'RelatedFieldOfStudy.txt'\n",
    "outputFile = 'RelatedTo.csv'\n",
    "\n",
    "inputFilePath = os.path.join(MAG_Data_Root, inputFile)\n",
    "outputFilePath = os.path.join(relationOutputRoot, outputFile)\n",
    "inputCSVHeader = RelatedFieldOfStudy_Table_Header\n",
    "outputCSVHeader = ['FieldOfStudyId', 'FieldOfStudyId', 'Rank']\n",
    "\n",
    "print('Start processing input file: {}'.format(inputFilePath))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "text2CSV(inputFilePath, outputFilePath, inputDelimiter, outputDelimiter, \n",
    "         inputCSVHeader, outputCSVHeader, parseRelatedTo)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "runtime = (end - start) / 60.0\n",
    "print('Elapsed time in min {}'.format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/PaperReferences.txt\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/PaperReferences.txt, bad lines: 0/0/1399752645\n",
      "Elapsed time in min 64.8411320567131\n"
     ]
    }
   ],
   "source": [
    "# generate reference relathonship\n",
    "\n",
    "inputFile = 'PaperReferences.txt'\n",
    "outputFile = 'References.csv'\n",
    "\n",
    "inputFilePath = os.path.join(MAG_Data_Root, inputFile)\n",
    "outputFilePath = os.path.join(relationOutputRoot, outputFile)\n",
    "inputCSVHeader = PaperReferences_Table_Header\n",
    "outputCSVHeader = ['PaperId', 'PaperId']\n",
    "\n",
    "print('Start processing input file: {}'.format(inputFilePath))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "text2CSV(inputFilePath, outputFilePath, inputDelimiter, outputDelimiter, \n",
    "         inputCSVHeader, outputCSVHeader, joinByDelimiter)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "runtime = (end - start) / 60.0\n",
    "print('Elapsed time in min {}'.format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/PaperRecommendations.txt\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/PaperRecommendations.txt, bad lines: 0/0/2535427126\n",
      "Elapsed time in min 134.97164742946626\n"
     ]
    }
   ],
   "source": [
    "# generate recommendations relathonship\n",
    "\n",
    "inputFile = 'PaperRecommendations.txt'\n",
    "outputFile = 'Recommendations.csv'\n",
    "\n",
    "inputFilePath = os.path.join(MAG_Data_Root, inputFile)\n",
    "outputFilePath = os.path.join(relationOutputRoot, outputFile)\n",
    "inputCSVHeader = PaperRecommendations_Table_Header\n",
    "outputCSVHeader = ['PaperId', 'PaperId', 'Weight']\n",
    "\n",
    "print('Start processing input file: {}'.format(inputFilePath))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "text2CSV(inputFilePath, outputFilePath, inputDelimiter, outputDelimiter, \n",
    "         inputCSVHeader, outputCSVHeader, joinByDelimiter)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "runtime = (end - start) / 60.0\n",
    "print('Elapsed time in min {}'.format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/PaperAuthorAffiliations.txt\n"
     ]
    }
   ],
   "source": [
    "# generate affiliatedWith and authorOf relathonships\n",
    "\n",
    "inputFile = 'PaperAuthorAffiliations.txt'\n",
    "affiliatedWithOutFile = 'AffiliatedWith.csv'\n",
    "authorOfOutFile = 'AuthorOf.csv'\n",
    "\n",
    "inputFilePath = os.path.join(MAG_Data_Root, inputFile)\n",
    "affiliatedWithOutFilePath = os.path.join(relationOutputRoot, affiliatedWithOutFile)\n",
    "authorOfOutFilePath = os.path.join(relationOutputRoot, authorOfOutFile)\n",
    "\n",
    "inputHeader = PaperAuthorAffiliation_Table_Header\n",
    "affiliatedWithHeader = ['AuthorId', 'AffiliationId']\n",
    "authorOfHeader = ['AuthorId', 'PaperId']\n",
    "\n",
    "\n",
    "print('Start processing input file: {}'.format(inputFilePath))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "parsePaperAuthorAffiliation(inputFilePath, inputHeader, \n",
    "                                affiliatedWithOutFilePath, authorOfOutFilePath,\n",
    "                               inputDelimiter, outputDelimiter,\n",
    "                               affiliatedWithHeader, authorOfHeader)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "runtime = (end - start) / 60.0\n",
    "print('Elapsed time in min {}'.format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/Papers.txt\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/Papers.txt, bad lines: 0/0/212209775\n",
      "Elapsed time in min 23.586532878875733\n"
     ]
    }
   ],
   "source": [
    "# generate publishedIn relathonship\n",
    "\n",
    "inputFile = 'Papers.txt'\n",
    "outputFile = 'PublishedIn.csv'\n",
    "\n",
    "inputFilePath = os.path.join(MAG_Data_Root, inputFile)\n",
    "outputFilePath = os.path.join(relationOutputRoot, outputFile)\n",
    "inputCSVHeader = [''] * 22\n",
    "outputCSVHeader = ['PaperId', 'JournalId']\n",
    "\n",
    "print('Start processing input file: {}'.format(inputFilePath))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "text2CSV(inputFilePath, outputFilePath, inputDelimiter, outputDelimiter, \n",
    "         inputCSVHeader, outputCSVHeader, parsePublishedIn)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "runtime = (end - start) / 60.0\n",
    "print('Elapsed time in min {}'.format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/Papers.txt\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/Papers.txt, bad lines: 0/0/212209775\n",
      "Elapsed time in min 22.71127060254415\n"
     ]
    }
   ],
   "source": [
    "# generate presentedAt relathonship\n",
    "\n",
    "inputFile = 'Papers.txt'\n",
    "outputFile = 'PresentedAt.csv'\n",
    "\n",
    "inputFilePath = os.path.join(MAG_Data_Root, inputFile)\n",
    "outputFilePath = os.path.join(relationOutputRoot, outputFile)\n",
    "inputCSVHeader = [''] * 22\n",
    "outputCSVHeader = ['PaperId', 'ConferenceInstanceId']\n",
    "\n",
    "print('Start processing input file: {}'.format(inputFilePath))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "text2CSV(inputFilePath, outputFilePath, inputDelimiter, outputDelimiter, \n",
    "         inputCSVHeader, outputCSVHeader, parsePresentedAt)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "runtime = (end - start) / 60.0\n",
    "print('Elapsed time in min {}'.format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/Affiliations.txt\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/Affiliations.txt, bad lines: 0/0/25454\n",
      "Elapsed time in min 0.0030842224756876626\n",
      "\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/Authors.txt\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/Authors.txt, bad lines: 0/0/255991186\n",
      "Elapsed time in min 17.10336611668269\n",
      "\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/Journals.txt\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/Journals.txt, bad lines: 0/0/48668\n",
      "Elapsed time in min 0.005201462904612223\n",
      "\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/ConferenceInstances.txt\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/ConferenceInstances.txt, bad lines: 0/0/15737\n",
      "Elapsed time in min 0.01074142853418986\n",
      "\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/ConferenceSeries.txt\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/ConferenceSeries.txt, bad lines: 0/0/4351\n",
      "Elapsed time in min 0.001696324348449707\n",
      "\n",
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/FieldsOfStudy.txt\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/FieldsOfStudy.txt, bad lines: 0/0/229847\n",
      "Elapsed time in min 0.022711447874704995\n"
     ]
    }
   ],
   "source": [
    "# process Nodes\n",
    "\n",
    "fileList = [('Affiliations.txt', 'Affiliation.csv', Affilication_Table_Header), \n",
    "           ('Authors.txt', 'Author.csv', Author_Table_Header),\n",
    "           ('Journals.txt', 'Journal.csv', Journal_Table_Header),\n",
    "           ('ConferenceInstances.txt', 'ConferenceInstance.csv', ConferenceInstance_Table_Header),\n",
    "           ('ConferenceSeries.txt', 'ConferenceSeries.csv', ConferenceSeries_Table_Header),\n",
    "           ('FieldsOfStudy.txt', 'FieldsOfStudy.csv', FieldOfStudy_Table_Header)]\n",
    "\n",
    "for entry in fileList:\n",
    "    inputFile = entry[0]\n",
    "    outputFile = entry[1]\n",
    "\n",
    "    inputFilePath = os.path.join(MAG_Data_Root, inputFile)\n",
    "    outputFilePath = os.path.join(outputRoot, outputFile)\n",
    "    csvHeader = entry[2]\n",
    "\n",
    "    print()\n",
    "    print('Start processing input file: {}'.format(inputFilePath))\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    text2CSV(inputFilePath, outputFilePath, inputDelimiter, outputDelimiter, csvHeader, csvHeader, joinByDelimiter)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    runtime = (end - start) / 60.0\n",
    "    print('Elapsed time in min {}'.format(runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing input file: /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/PaperAbstractsInvertedIndex.txt\n",
      "Finished processing /N/dc2/projects/rds/gruan/workspace/CADRE/data/mag/PaperAbstractsInvertedIndex.txt, bad lines: 0/142625/95842254\n",
      "Elapsed time in min 205.3731875737508\n"
     ]
    }
   ],
   "source": [
    "# process inverted index\n",
    "\n",
    "inputFile = 'PaperAbstractsInvertedIndex.txt'\n",
    "outputFile = 'PaperAbstract.csv'\n",
    "\n",
    "inputFilePath = os.path.join(MAG_Data_Root, inputFile)\n",
    "outputFilePath = os.path.join(outputRoot, outputFile)\n",
    "csvHeader = PaperAbstract_Table_Header\n",
    "\n",
    "print('Start processing input file: {}'.format(inputFilePath))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "text2CSV(inputFilePath, outputFilePath, inputDelimiter, outputDelimiter, \n",
    "         csvHeader, csvHeader, invertedIdx2Abstract)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "runtime = (end - start) / 60.0\n",
    "print('Elapsed time in min {}'.format(runtime))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
